<!doctype html><html><head><title>Copilots - Dr. Chee Wei Tan - Computer Science</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Dr. Chee Wei Tan - Researcher in network optimization, distributed machine learning, cloud computing, and AI. M.A and Ph.D. in Electrical Engineering from Princeton University. Explore his portfolio and expertise."><meta property="og:title" content=Copilots><meta property="og:description" content="Dr. Chee Wei Tan - Researcher in network optimization, distributed machine learning, cloud computing, and AI. M.A and Ph.D. in Electrical Engineering from Princeton University. Explore his portfolio and expertise."><meta name=twitter:title content=Copilots><meta name=twitter:description content="Dr. Chee Wei Tan - Researcher in network optimization, distributed machine learning, cloud computing, and AI. M.A and Ph.D. in Electrical Engineering from Princeton University. Explore his portfolio and expertise."><link rel=canonical href=https://cheewtan.github.io/copilots/><link href="https://fonts.googleapis.com/css?family=Noto+Sans" rel=stylesheet><link href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet><link rel=stylesheet href=https://cheewtan.github.io/css/styles.css></head><body class="flex md:flex-row-reverse flex-col-reverse items-center md:items-start justify-start bg-gradient-to-r from-slate-50 via-main-50 to-main-50"><div class="w-[fill-available] h-[fill-available] md:h-auto flex justify-center"><div id=content class="flex flex-col items-center max-w-[800px] md:w-5/6 pb-20 md:pt-12 px-8"><div class="article mt-0"><h2 class="text-3xl font-bold text-center">Large Language Models and Code Copilots for AI-Assisted Programming</h2><h3 id=introduction>Introduction</h3><p>This tutorial delves into the evolution of AI-assisted programming, tracing its roots to E.W.Dijkstra&rsquo;s seminal idea of computer-assisted programming and to Natural Language Processing (NLP) and probabilistic language models. It highlights the recent transformative impact of modern transformerbased large language models (LLMs) trained on Big Code, leveraging software naturalness to revolutionize tasks like code generation, completion, translation, and defect detection. Pioneering examples include GitHub Copilot (powered by OpenAI Codex), GPT models, Meta&rsquo;s Code Llama, Google&rsquo;s Gemini Code Assist, Amazon CodeWhisperer, Alibaba&rsquo;s Qwen, and Codeium.</p><p>Participants will explore advancements in contextual-aware, multilingual programming models that enhance the adaptability of both local and cloud-based LLMs in diverse ecosystems. Core LLM architectures, their downstream applications, and challenges in integrating NLP methodologies with software naturalness will be examined. The tutorial highlights reinforcement learning with human feedback, focusing on alignment techniques to enhance fairness, safety, and performance in code generation by large language models. The session demonstrates AI-assisted programming extensions to Apple&rsquo;s Xcode and LLM agent development, showcasing tools like Copilot to streamline mobile development and empower participants to evaluate, benchmark, and deploy LLMs effectively.</p><p>The tutorial will also focus on general techniques for benchmarking and evaluation of LLMs for AIassisted programming. Models are assessed using code-specific benchmarks such as HumanEval and CodeNet, providing standardized datasets for evaluating code generation and completion. Performance metrics like Pass@k, BLEU, CodeBLEU, and functional correctness are analyzed to quantify the quality of generated code. Real-world effectiveness is gauged through human evaluations and deployment case studies, which provide valuable insights into user experiences and practical challenges.</p><p>Additionally, advanced evaluation methodologies are discussed, including fine-grained analysis to identify common errors, assess model robustness, and measure performance on adversarial inputs. Comparative studies across different programming languages and domains illustrate the adaptability and limitations of various models, including emerging LLM coding agent players, which demonstrate cutting-edge advancements in multilingual programming and cross-domain functionality.</p><p>Lastly, LLMs and LLM agents have profound implications for computer science, driving advancements in the search for efficient algorithms and automating problem-solving in competitive programming. By tackling complex programming challenges, they open new avenues for understanding algorithm design, optimization, and the theoretical foundations of computation.</p><h3 id=tutorial-speakers>Tutorial Speakers</h3><div class="flex flex-row items-start space-x-4"><img src=/images/cheetan.jpg alt="Chee Wei Tan" class="w-[100px] h-[150px]"><div class="flex flex-col items-start"><h4 class=font-bold>Chee Wei Tan</h4><p>Dr. Tan received the M.A and Ph.D. degrees in Electrical Engineering from <a href=http://www.princeton.edu/>Princeton University</a>. He is currently with College of Computing and Data Science, Nanyang Technological University in Singapore. He was a postdoctoral scholar in the <a href=http://netlab.caltech.edu/>NetLab Group</a> at Caltech, a senior fellow for Science at Extreme Scales program at the Institute for Pure and Applied Mathematics at UCLA, and was a visiting faculty at Tencent AI Lab and Qualcomm R&D (QRC). His research interests are distributed optimization, Generative AI, networks and edge learning.</p></div></div><br><div class="flex flex-row items-start space-x-4"><img src=/images/yuchenwang.jpg alt="Yuchen Wang" class="w-[100px] h-[150px]"><div class="flex flex-col items-start"><h4 class=font-bold>Yuchen Wang</h4><p>Yuchen Wang is a Ph.D. candidate at Nanyang Technological University and a full-stack software engineer specializing in mobile development. She received her B.E. and M.S. degrees in Computer Science, both with Honours with the Highest Distinction, from the National University of Singapore. Her research mainly focuses on AI-Assisted Programming and Sound and Music Computing.</p></div></div><h3 id=affiliation>Affiliation</h3><p>NTU Singapore</p><h3 id=relevant-links>Relevant Links</h3><ul><li><a href=https://arxiv.org/abs/2107.03374>Evaluating Large Language Models Trained on Code</a></li><li><a href=https://arxiv.org/pdf/2305.06161>StarCoder: may the source be with you!</a></li><li><a href=https://arxiv.org/abs/2308.12950>Code Llama: Open Foundation Models for Code</a></li><li><a href=https://arxiv.org/abs/2401.14196>DeepSeek-Coder: When the Large Language Model Meets Programming &ndash; The Rise of Code Intelligence</a></li><li><a href=https://arxiv.org/abs/2410.15285>Contextual Augmented Multi-Model Programming (CAMP): A Hybrid Local-Cloud Copilot Framework</a></li><li><a href=https://arxiv.org/abs/2307.02503>Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review</a></li><li><a href=https://arxiv.org/abs/2307.14349>Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models</a></li><li><a href=https://ieeexplore.ieee.org/document/10818581>Aligning Crowd-Sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models</a></li></ul><h3 id=slides>Slides</h3><p>Coming soon.</p></div><footer></footer></div></div><div class="w-[100%] md:w-[16%] md:min-w-[180px] md:max-w-[240px] md:h-[min-content] sticky top-0"><div class="md:hidden px-8 relative py-4"><script>let open=!1;document.addEventListener("click",e=>{const t=document.getElementById("togglable-menu");if(!open)return;if(e.target===t)return;t.style.opacity=0,open=!1});function toggleMenu(){const e=document.getElementById("togglable-menu");e.style.opacity<=0?(e.style.opacity=100,setTimeout(()=>{open=!0},200)):(e.style.opacity=0,open=!1)}</script><button class="text-2xl text-white bg-black w-[40px] h-[40px]" onclick=toggleMenu()>
<i class="fa fa-regular fa-bars"></i></button><div id=togglable-menu class="flex flex-col gap-6 absolute left-6 top-[100%] p-4 bg-black text-slate-300 transition-opacity duration-200 opacity-0 shadow-lg max-h-[80vh] overflow-y-scroll"><div><div class="flex flex-col gap-2 mb-6"><div class="flex flex-col gap-2 text-base"><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/>Home
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/publications>Publications
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/research>Research
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/teaching-and-outreach>Teaching</a></div></div></div></div></div><div class="hidden h-[100vh] text-slate-300 md:flex flex-col gap-6 pt-20 px-6 bg-black overflow-y-scroll pb-8"><div><div class="flex flex-col gap-2 mb-6"><div class="flex flex-col gap-2 text-base"><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/>Home
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/publications>Publications
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/research>Research
</a><a class="hover:text-slate-50 transition-all duration-100 ease-in-out hover:translate-x-1" href=/teaching-and-outreach>Teaching</a></div></div></div></div></div></body></html>